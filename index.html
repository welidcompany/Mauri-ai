<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ</title>
  <link href="https://fonts.googleapis.com/css2?family=Changa&display=swap" rel="stylesheet" />
  <style>
    * { margin:0; padding:0; box-sizing:border-box }
    body {
      font-family:'Changa',sans-serif;
      background:linear-gradient(135deg,#e3f2fd,#bbdefb);
      display:flex;align-items:center;justify-content:center;
      min-height:100vh;
    }
    .card {
      background:rgba(255,255,255,0.9);
      border-radius:20px;
      box-shadow:0 8px 24px rgba(0,0,0,0.1);
      max-width:600px; width:100%; text-align:center;
      padding:30px 20px;
    }
    #avatar {
      width:350px; height:350px;
      border-radius:50%; border:6px solid #42a5f5;
      object-fit:cover; margin-bottom:20px;
      transition:transform .2s;
    }
    .speaking #avatar { animation:lip-sync .4s infinite }
    .nodding  #avatar { animation:head-nod .8s infinite }
    @keyframes lip-sync  {0%,100%{transform:scaleY(1)}50%{transform:scaleY(.85)}}
    @keyframes head-nod  {0%,100%{transform:rotate(0deg)}50%{transform:rotate(4deg)}}
    #mic {
      background:#42a5f5;border:none;
      width:80px;height:80px;border-radius:50%;
      display:inline-flex;align-items:center;justify-content:center;
      cursor:pointer;box-shadow:0 4px 16px rgba(0,0,0,.2);
      transition:background .3s,transform .2s;margin-top:10px;
    }
    #mic:hover {background:#1e88e5;transform:scale(1.05)}
    #mic:active{transform:scale(.95)}
    #mic svg {width:36px;height:36px;fill:#fff}
    #response {
      margin-top:20px;min-height:80px;
      padding:15px;background:#f1f1f1;
      border-radius:12px;font-size:18px;
      line-height:1.4;color:#424242;
    }
    #note {margin-top:15px;font-size:14px;color:#757575}
  </style>
</head>
<body>
  <div class="card">
    <img id="avatar" src="avatar.jpg" alt="Avatar">
    <button id="mic" aria-label="Start/Stop recording">
      <svg viewBox="0 0 24 24">
        <path d="M12 14a3 3 0 0 0 3-3V5a3 3 0 0 0-6 0v6a3 3 0 0 0 3 3zm5-3a5 5 0 0 1-10 0H5a7 7 0 0 0 14 0h-2zM11 19.93V22h2v-2.07a8.001 8.001 0 0 1-2 0z"/>
      </svg>
    </button>
    <div id="response">Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø± ÙˆØ§Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„...</div>
    <div id="note">âš ï¸ ÙŠØ¹Ù…Ù„ ÙÙ‚Ø· Ø¹Ø¨Ø± HTTPS (GitHub Pages).</div>
  </div>

  <script type="module">
    import OpenAI from 'https://esm.sh/openai';

    const OPENAI_API_KEY = 'sk-â€¦';  // Ø¶Ø¹ Ù…ÙØªØ§Ø­Ùƒ Ù‡Ù†Ø§

    const client = new OpenAI({ apiKey: OPENAI_API_KEY });
    const micBtn = document.getElementById('mic');
    const resp   = document.getElementById('response');
    const avatar = document.getElementById('avatar');

    let recorder, chunks = [], recording = false;

    micBtn.addEventListener('click', async () => {
      if (!recording) {
        // Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        try {
          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
          recorder = new MediaRecorder(stream);
          chunks = [];
          recorder.ondataavailable = e => chunks.push(e.data);
          recorder.onstop = onStopRecording;
          recorder.start();
          recording = true;
          resp.textContent = 'ğŸ”´ Ø¬Ø§Ø±Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„... Ø§Ø¶ØºØ· Ù„Ø¥ÙŠÙ‚Ø§Ù';
          micBtn.style.background = '#e53935';
        } catch {
          resp.textContent = 'âŒ Ù„Ù… ÙŠØªÙ… Ù…Ù†Ø­ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.';
        }
      } else {
        // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        recorder.stop();
        recording = false;
        resp.textContent = 'â³ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ...';
        micBtn.style.background = '#42a5f5';
      }
    });

    async function onStopRecording() {
      const blob = new Blob(chunks, { type: 'audio/webm' });
      // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Whisper
      const fd = new FormData();
      fd.append('file', blob, 'voice.webm');
      fd.append('model', 'whisper-1');
      const whisperRes = await fetch('https://api.openai.com/v1/audio/transcriptions', {
        method: 'POST',
        headers: { 'Authorization': `Bearer ${OPENAI_API_KEY}` },
        body: fd
      });
      const whisperData = await whisperRes.json();
      const userText = whisperData.text;
      resp.textContent = `ğŸ“ Ø£Ù†Øª: ${userText}`;
      // Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
      resp.textContent = 'â³ Ø£Ø¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...';
      try {
        const chat = await client.responses.create({
          model: 'gpt-4.1',
          input: userText
        });
        const answer = chat.output_text.trim();
        resp.textContent = answer;
        speak(answer);
      } catch {
        resp.textContent = 'âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯.';
      }
    }

    function speak(text) {
      avatar.classList.add('nodding');
      const u = new SpeechSynthesisUtterance(text);
      u.lang = 'ar-SA';
      u.onstart = () => avatar.classList.add('speaking');
      u.onend   = () => {
        avatar.classList.remove('speaking');
        avatar.classList.remove('nodding');
      };
      speechSynthesis.speak(u);
    }
  </script>
</body>
</html>
