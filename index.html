<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ</title>
  <link href="https://fonts.googleapis.com/css2?family=Changa&display=swap" rel="stylesheet" />
  <style>
    * { margin:0; padding:0; box-sizing:border-box; }
    body {
      font-family: 'Changa', sans-serif;
      background: linear-gradient(135deg,#e3f2fd,#bbdefb);
      display:flex; align-items:center; justify-content:center;
      min-height:100vh;
    }
    .card {
      background:rgba(255,255,255,0.9);
      border-radius:20px;
      box-shadow:0 8px 24px rgba(0,0,0,0.1);
      max-width:600px; width:100%;
      text-align:center; padding:30px 20px;
    }
    #avatar {
      width:350px; height:350px;
      border-radius:50%; border:6px solid #42a5f5;
      object-fit:cover; margin-bottom:20px;
      transition:transform .2s;
    }
    .speaking #avatar { animation:lip-sync .4s infinite; }
    .nodding  #avatar { animation:head-nod .8s infinite; }
    @keyframes lip-sync {
      0%,100% {transform:scaleY(1);} 50% {transform:scaleY(.85);}
    }
    @keyframes head-nod {
      0%,100% {transform:rotate(0deg);} 50% {transform:rotate(4deg);}
    }
    #mic {
      background:#42a5f5; border:none;
      width:80px; height:80px; border-radius:50%;
      display:inline-flex; align-items:center;
      justify-content:center; cursor:pointer;
      box-shadow:0 4px 16px rgba(0,0,0,.2);
      transition:background .3s,transform .2s;
    }
    #mic:hover { background:#1e88e5; transform:scale(1.05); }
    #mic:active{ transform:scale(.95); }
    #mic svg { width:36px; height:36px; fill:#fff; }
    #response {
      margin-top:20px; min-height:80px;
      padding:15px; background:#f1f1f1;
      border-radius:12px; font-size:18px;
      line-height:1.4; color:#424242;
    }
    #note {
      margin-top:15px; font-size:14px; color:#757575;
    }
  </style>
</head>
<body>
  <div class="card">
    <img id="avatar" src="avatar.jpg" alt="Avatar">
    <button id="mic" aria-label="Start listening">
      <svg viewBox="0 0 24 24">
        <path d="M12 14a3 3 0 0 0 3-3V5a3 3 0 0 0-6 0v6a3 3 0 0 0 3 3zm5-3a5 5 0 0 1-10 0H5a7 7 0 0 0 14 0h-2zM11 19.93V22h2v-2.07a8.001 8.001 0 0 1-2 0z"/>
      </svg>
    </button>
    <div id="response">Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø± ÙˆØ§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ùƒ...</div>
    <div id="note">âš ï¸ ÙŠØ¬Ø¨ ÙØªØ­ Ø¹Ø¨Ø± HTTPS (GitHub Pages).</div>
  </div>

  <script type="module">
    import OpenAI from 'https://esm.sh/openai';

    // Ø¶Ø¹ Ù…ÙØªØ§Ø­Ùƒ Ù‡Ù†Ø§ (Ø§Ø­ÙØ¸Ù‡ Ù…Ø­Ù„ÙŠØ§Ù‹ ÙˆÙ„Ø§ ØªØ±ÙØ¹Ù‡ Ø¹Ù„Ù†Ù‹Ø§)
    const OPENAI_API_KEY = 'sk-â€¦';

    // polyfill Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¯Ø¹Ù… SpeechRecognition
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      document.getElementById('note').textContent = 'Ø§Ù„Ù…ØªØµÙØ­ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª.';
      throw new Error('SpeechRecognition not supported');
    }

    const recognition = new SpeechRecognition();
    recognition.lang = 'ar-SA';
    recognition.interimResults = false;
    recognition.maxAlternatives = 1;

    const micBtn = document.getElementById('mic');
    const resp   = document.getElementById('response');
    const avatar = document.getElementById('avatar');

    // Ø¹Ù…ÙŠÙ„ OpenAI
    const client = new OpenAI({ apiKey: OPENAI_API_KEY });

    micBtn.addEventListener('click', () => {
      resp.textContent = 'ğŸ”Š Ø§Ø³ØªÙ…Ø¹...';
      recognition.start();
    });

    recognition.addEventListener('start', () => {
      micBtn.disabled = true;
    });
    recognition.addEventListener('end', () => {
      micBtn.disabled = false;
    });

    recognition.addEventListener('result', async evt => {
      const text = evt.results[0][0].transcript;
      resp.textContent = 'â³ Ø¬Ø§Ø±Ù Ø§Ù„ØªÙÙƒÙŠØ±...';
      try {
        const response = await client.responses.create({
          model: 'gpt-4.1',
          input: text
        });
        const answer = response.output_text.trim();
        resp.textContent = answer;
        speak(answer);
      } catch {
        resp.textContent = 'âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ OpenAI.';
      }
    });

    recognition.addEventListener('error', evt => {
      resp.textContent = 'âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª.';
      console.error(evt.error);
    });

    function speak(text) {
      avatar.classList.add('nodding');
      const u = new SpeechSynthesisUtterance(text);
      u.lang = 'ar-SA';
      u.onstart = () => avatar.classList.add('speaking');
      u.onend   = () => {
        avatar.classList.remove('speaking');
        avatar.classList.remove('nodding');
      };
      speechSynthesis.speak(u);
    }
  </script>
</body>
</html>
