<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ</title>
  <link href="https://fonts.googleapis.com/css2?family=Changa&display=swap" rel="stylesheet" />
  <style>
    * { margin:0; padding:0; box-sizing:border-box }
    body {
      font-family:'Changa',sans-serif;
      background:linear-gradient(135deg,#e3f2fd,#bbdefb);
      display:flex;align-items:center;justify-content:center;
      min-height:100vh;
    }
    .card {
      background:rgba(255,255,255,0.9);
      border-radius:20px;
      box-shadow:0 8px 24px rgba(0,0,0,0.1);
      max-width:600px; width:100%; text-align:center;
      padding:30px 20px;
    }
    #avatar {
      width:350px; height:350px;
      border-radius:50%; border:6px solid #42a5f5;
      object-fit:cover; margin-bottom:20px;
      transition:transform .2s;
    }
    .speaking #avatar { animation:lip-sync .4s infinite }
    .nodding  #avatar { animation:head-nod .8s infinite }
    @keyframes lip-sync  {0%,100%{transform:scaleY(1)}50%{transform:scaleY(.85)}}
    @keyframes head-nod  {0%,100%{transform:rotate(0deg)}50%{transform:rotate(4deg)}}
    #mic {
      background:#42a5f5;border:none;
      width:80px;height:80px;border-radius:50%;
      display:inline-flex;align-items:center;justify-content:center;
      cursor:pointer;box-shadow:0 4px 16px rgba(0,0,0,.2);
      transition:background .3s,transform .2s;margin-top:10px;
    }
    #mic:hover {background:#1e88e5;transform:scale(1.05)}
    #mic:active{transform:scale(.95)}
    #mic svg {width:36px;height:36px;fill:#fff}
    #response {
      margin-top:20px;min-height:80px;
      padding:15px;background:#f1f1f1;
      border-radius:12px;font-size:18px;
      line-height:1.4;color:#424242;
    }
    #note {margin-top:15px;font-size:14px;color:#757575}
  </style>
</head>
<body>
  <div class="card">
    <img id="avatar" src="avatar.jpg" alt="Avatar">
    <button id="mic" aria-label="Start/Stop recording">
      <svg viewBox="0 0 24 24">
        <path d="M12 14a3 3 0 0 0 3-3V5a3 3 0 0 0-6 0v6a3 3 0 0 0 3 3zm5-3a5 5 0 0 1-10 0H5a7 7 0 0 0 14 0h-2zM11 19.93V22h2v-2.07a8.001 8.001 0 0 1-2 0z"/>
      </svg>
    </button>
    <div id="response">Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ø²Ø± ÙˆØ§Ø¨Ø¯Ø£ Ø§Ù„ØªØ³Ø¬ÙŠÙ„...</div>
    <div id="note">âš ï¸ ÙŠØ¹Ù…Ù„ ÙÙ‚Ø· Ø¹Ø¨Ø± HTTPS (GitHub Pages).</div>
  </div>

  <!-- OpenAI UMD client -->
  <script src="https://unpkg.com/openai@3.3.0/dist/openai.umd.js"></script>
  <script>
    const OPENAI_API_KEY = 'sk-â€¦'; // Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­Ùƒ Ù‡Ù†Ø§
    const client = new OpenAI.OpenAI({ apiKey: OPENAI_API_KEY });

    let recorder, chunks = [], recording = false;

    // Ø¹Ù†Ø¯ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: Ø·Ù„Ø¨ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙÙˆØ±Ù‹Ø§
    window.addEventListener('load', async () => {
      try {
        await navigator.mediaDevices.getUserMedia({ audio: true });
        document.getElementById('note').textContent = 'âœ… ØªÙ… ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†ØŒ Ø§Ø¶ØºØ· Ø§Ù„Ø²Ø± Ù„Ù„ØªØ³Ø¬ÙŠÙ„.';
      } catch (e) {
        document.getElementById('note').textContent = 'âŒ ØªÙ… Ø±ÙØ¶ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.';
        console.error(e);
      }
    });

    document.getElementById('mic').addEventListener('click', () => {
      const resp = document.getElementById('response');
      if (!recording) {
        // Ø¨Ø¯Ø¡ Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
          recorder = new MediaRecorder(stream);
          chunks = [];
          recorder.ondataavailable = e => chunks.push(e.data);
          recorder.onstop = onStopRecording;
          recorder.start();
          recording = true;
          resp.textContent = 'ğŸ”´ Ø¬Ø§Ø±Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„... Ø§Ø¶ØºØ· Ù„Ø¥ÙŠÙ‚Ø§Ù';
          document.getElementById('mic').style.background = '#e53935';
        }).catch(err => {
          resp.textContent = 'âŒ Ù„Ù… ÙŠØªÙ… Ù…Ù†Ø­ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.';
          console.error(err);
        });
      } else {
        // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        recorder.stop();
        recording = false;
        resp.textContent = 'â³ ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¥Ù„Ù‰ Ù†Øµ...';
        document.getElementById('mic').style.background = '#42a5f5';
      }
    });

    async function onStopRecording() {
      const blob = new Blob(chunks, { type: 'audio/webm' });
      // Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ Whisper API
      const form = new FormData();
      form.append('file', blob, 'voice.webm');
      form.append('model', 'whisper-1');
      try {
        const whRes = await fetch('https://api.openai.com/v1/audio/transcriptions', {
          method: 'POST',
          headers: { 'Authorization': `Bearer ${OPENAI_API_KEY}` },
          body: form
        });
        const whData = await whRes.json();
        const userText = whData.text;
        document.getElementById('response').textContent = `ğŸ“ Ø£Ù†Øª: ${userText}`;
        // Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ GPT-4.1
        document.getElementById('response').textContent = 'â³ Ø£Ø¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...';
        const chat = await client.responses.create({
          model: 'gpt-4.1',
          input: userText
        });
        const answer = chat.output_text.trim();
        document.getElementById('response').textContent = answer;
        speak(answer);
      } catch (err) {
        document.getElementById('response').textContent = 'âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.';
        console.error(err);
      }
    }

    function speak(text) {
      const avatar = document.getElementById('avatar');
      avatar.classList.add('nodding');
      const u = new SpeechSynthesisUtterance(text);
      u.lang = 'ar-SA';
      u.onstart = () => avatar.classList.add('speaking');
      u.onend   = () => {
        avatar.classList.remove('speaking');
        avatar.classList.remove('nodding');
      };
      speechSynthesis.speak(u);
    }
  </script>
</body>
</html>
